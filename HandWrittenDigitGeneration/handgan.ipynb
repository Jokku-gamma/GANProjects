{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fXPQuwVtXsef"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "TensorFlow Version: 2.19.0\n",
            "Train images shape: (60000, 28, 28, 1)\n",
            "Starting Epoch 0\n",
            "Batch 0, D Loss: 0.7201184034347534, G Loss: 0.7203408479690552\n",
            "Batch 50, D Loss: 0.38894522190093994, G Loss: 0.3871029019355774\n",
            "Batch 100, D Loss: 0.3473746180534363, G Loss: 0.3934482932090759\n",
            "Batch 150, D Loss: 0.2261328101158142, G Loss: 0.665773332118988\n",
            "Batch 200, D Loss: 0.10789243131875992, G Loss: 1.1203272342681885\n",
            "Batch 250, D Loss: 0.11313021183013916, G Loss: 0.7053894996643066\n",
            "Batch 300, D Loss: 0.21448953449726105, G Loss: 0.7832954525947571\n",
            "Batch 350, D Loss: 0.4642738401889801, G Loss: 0.9528861045837402\n",
            "Batch 400, D Loss: 0.469396710395813, G Loss: 1.2348098754882812\n",
            "Batch 450, D Loss: 0.49176424741744995, G Loss: 1.1701765060424805\n",
            "Batch 500, D Loss: 0.5506314039230347, G Loss: 1.4121313095092773\n",
            "Batch 550, D Loss: 0.5133316516876221, G Loss: 0.8789684772491455\n",
            "Batch 600, D Loss: 0.49538981914520264, G Loss: 0.9567239880561829\n",
            "Batch 650, D Loss: 0.5319229364395142, G Loss: 0.9006770849227905\n",
            "Batch 700, D Loss: 0.5736928582191467, G Loss: 0.9882814884185791\n",
            "Batch 750, D Loss: 0.6633625626564026, G Loss: 0.7814592123031616\n",
            "Batch 800, D Loss: 0.6358211040496826, G Loss: 0.9655240774154663\n",
            "Batch 850, D Loss: 0.5800378322601318, G Loss: 0.8756016492843628\n",
            "Batch 900, D Loss: 0.7490155696868896, G Loss: 0.9118406772613525\n",
            "Batch 950, D Loss: 0.6461844444274902, G Loss: 0.6900615096092224\n",
            "Batch 1000, D Loss: 0.6630189418792725, G Loss: 0.7043750882148743\n",
            "Batch 1050, D Loss: 0.6722714900970459, G Loss: 0.7563399076461792\n",
            "Batch 1100, D Loss: 0.6304018497467041, G Loss: 0.7592452168464661\n",
            "Batch 1150, D Loss: 0.6962292194366455, G Loss: 0.8418698310852051\n",
            "Batch 1200, D Loss: 0.6541671752929688, G Loss: 0.8147342205047607\n",
            "Batch 1250, D Loss: 0.6456965208053589, G Loss: 0.8381535410881042\n",
            "Batch 1300, D Loss: 0.6369947195053101, G Loss: 0.7457438707351685\n",
            "Batch 1350, D Loss: 0.6666980981826782, G Loss: 0.8332154750823975\n",
            "Batch 1400, D Loss: 0.631706953048706, G Loss: 0.7483170032501221\n",
            "Batch 1450, D Loss: 0.5921570062637329, G Loss: 0.8790720701217651\n",
            "Batch 1500, D Loss: 0.6289652585983276, G Loss: 0.8541374206542969\n",
            "Batch 1550, D Loss: 0.65545654296875, G Loss: 0.8490505218505859\n",
            "Batch 1600, D Loss: 0.6668040156364441, G Loss: 0.7413155436515808\n",
            "Batch 1650, D Loss: 0.6944096684455872, G Loss: 0.7833704948425293\n",
            "Batch 1700, D Loss: 0.6309267282485962, G Loss: 0.8626406192779541\n",
            "Batch 1750, D Loss: 0.685822069644928, G Loss: 0.8941648006439209\n",
            "Batch 1800, D Loss: 0.6554831266403198, G Loss: 0.6910140514373779\n",
            "Batch 1850, D Loss: 0.6574753522872925, G Loss: 0.8015822768211365\n",
            "Epoch 0, D Loss: 0.65139240026474, G Loss: 0.8300024271011353\n",
            "Starting Epoch 1\n",
            "Batch 0, D Loss: 0.661709725856781, G Loss: 0.7470964193344116\n",
            "Batch 50, D Loss: 0.6276366710662842, G Loss: 0.9361259937286377\n",
            "Batch 100, D Loss: 0.6266646385192871, G Loss: 0.8138054609298706\n",
            "Batch 150, D Loss: 0.641406774520874, G Loss: 0.8169496059417725\n",
            "Batch 200, D Loss: 0.6560764312744141, G Loss: 0.8789896368980408\n",
            "Batch 250, D Loss: 0.6773632764816284, G Loss: 0.7831451892852783\n",
            "Batch 300, D Loss: 0.6579700112342834, G Loss: 0.7966806292533875\n",
            "Batch 350, D Loss: 0.6469516158103943, G Loss: 0.8113072514533997\n",
            "Batch 400, D Loss: 0.6568664312362671, G Loss: 0.7838667631149292\n",
            "Batch 450, D Loss: 0.6512051224708557, G Loss: 0.8104232549667358\n",
            "Batch 500, D Loss: 0.6378908157348633, G Loss: 0.7786223888397217\n",
            "Batch 550, D Loss: 0.680587887763977, G Loss: 0.7562268376350403\n",
            "Batch 600, D Loss: 0.658538281917572, G Loss: 0.7872837781906128\n",
            "Batch 650, D Loss: 0.6463257074356079, G Loss: 0.9403390288352966\n",
            "Batch 700, D Loss: 0.6384238600730896, G Loss: 0.7960218191146851\n",
            "Batch 750, D Loss: 0.6294588446617126, G Loss: 0.801053524017334\n",
            "Batch 800, D Loss: 0.6657986640930176, G Loss: 0.7258981466293335\n",
            "Batch 850, D Loss: 0.639016330242157, G Loss: 0.8021397590637207\n",
            "Batch 900, D Loss: 0.6810009479522705, G Loss: 0.7861469984054565\n",
            "Batch 950, D Loss: 0.6159189939498901, G Loss: 0.8881310224533081\n",
            "Batch 1000, D Loss: 0.6063045263290405, G Loss: 0.747321367263794\n",
            "Batch 1050, D Loss: 0.6245632767677307, G Loss: 0.8981442451477051\n",
            "Batch 1100, D Loss: 0.6131489276885986, G Loss: 0.7792442440986633\n",
            "Batch 1150, D Loss: 0.5874231457710266, G Loss: 0.778079628944397\n",
            "Batch 1200, D Loss: 0.6501444578170776, G Loss: 0.8529813289642334\n",
            "Batch 1250, D Loss: 0.5932469367980957, G Loss: 0.8307807445526123\n",
            "Batch 1300, D Loss: 0.5806775093078613, G Loss: 0.8892952799797058\n",
            "Batch 1350, D Loss: 0.603431224822998, G Loss: 0.8030589818954468\n",
            "Batch 1400, D Loss: 0.6109097003936768, G Loss: 0.7078079581260681\n",
            "Batch 1450, D Loss: 0.5943450927734375, G Loss: 0.7850345969200134\n",
            "Batch 1500, D Loss: 0.6156971454620361, G Loss: 0.9343376159667969\n",
            "Batch 1550, D Loss: 0.6138184070587158, G Loss: 0.887270450592041\n",
            "Batch 1600, D Loss: 0.6409856677055359, G Loss: 0.7371479272842407\n",
            "Batch 1650, D Loss: 0.6114988327026367, G Loss: 0.846289873123169\n",
            "Batch 1700, D Loss: 0.6381080746650696, G Loss: 0.8864527344703674\n",
            "Batch 1750, D Loss: 0.6034502983093262, G Loss: 0.7136034965515137\n",
            "Batch 1800, D Loss: 0.5727084875106812, G Loss: 0.8501670360565186\n",
            "Batch 1850, D Loss: 0.6195480823516846, G Loss: 0.8406106233596802\n",
            "Epoch 1, D Loss: 0.5963549613952637, G Loss: 0.8815494775772095\n",
            "Starting Epoch 2\n",
            "Batch 0, D Loss: 0.653714656829834, G Loss: 0.7310560345649719\n",
            "Batch 50, D Loss: 0.6032747626304626, G Loss: 0.8977919220924377\n",
            "Batch 100, D Loss: 0.5447662472724915, G Loss: 0.8619034290313721\n",
            "Batch 150, D Loss: 0.6208961606025696, G Loss: 0.8599000573158264\n",
            "Batch 200, D Loss: 0.5633767247200012, G Loss: 0.8805075287818909\n",
            "Batch 250, D Loss: 0.6249129772186279, G Loss: 0.8377743363380432\n",
            "Batch 300, D Loss: 0.5439803600311279, G Loss: 0.9678972959518433\n",
            "Batch 350, D Loss: 0.6242014765739441, G Loss: 0.8691284656524658\n",
            "Batch 400, D Loss: 0.5978235602378845, G Loss: 0.8195571899414062\n",
            "Batch 450, D Loss: 0.6137187480926514, G Loss: 0.7134712934494019\n",
            "Batch 500, D Loss: 0.5366320013999939, G Loss: 0.9980564713478088\n",
            "Batch 550, D Loss: 0.6711443066596985, G Loss: 0.9126442670822144\n",
            "Batch 600, D Loss: 0.5444576740264893, G Loss: 0.9986220598220825\n",
            "Batch 650, D Loss: 0.584644615650177, G Loss: 0.8396321535110474\n",
            "Batch 700, D Loss: 0.6398688554763794, G Loss: 0.8098676204681396\n",
            "Batch 750, D Loss: 0.5422844886779785, G Loss: 0.9809609055519104\n",
            "Batch 800, D Loss: 0.5107333064079285, G Loss: 0.9206850528717041\n",
            "Batch 850, D Loss: 0.5424187183380127, G Loss: 0.9367327690124512\n",
            "Batch 900, D Loss: 0.6471017599105835, G Loss: 0.8464707136154175\n",
            "Batch 950, D Loss: 0.5833851099014282, G Loss: 0.7919796705245972\n",
            "Batch 1000, D Loss: 0.6230491399765015, G Loss: 0.9047574996948242\n",
            "Batch 1050, D Loss: 0.5568886399269104, G Loss: 0.7894296050071716\n",
            "Batch 1100, D Loss: 0.6864356994628906, G Loss: 0.7406140565872192\n",
            "Batch 1150, D Loss: 0.5685867071151733, G Loss: 0.7273380756378174\n",
            "Batch 1200, D Loss: 0.67631995677948, G Loss: 0.7010027766227722\n",
            "Batch 1250, D Loss: 0.6045587658882141, G Loss: 0.8017703294754028\n",
            "Batch 1300, D Loss: 0.5687571167945862, G Loss: 0.7591609954833984\n",
            "Batch 1350, D Loss: 0.6617732048034668, G Loss: 0.8785676956176758\n",
            "Batch 1400, D Loss: 0.6516338586807251, G Loss: 0.7564314603805542\n",
            "Batch 1450, D Loss: 0.6175237894058228, G Loss: 0.869584858417511\n",
            "Batch 1500, D Loss: 0.5991963148117065, G Loss: 0.708396315574646\n",
            "Batch 1550, D Loss: 0.6209105253219604, G Loss: 0.8136962056159973\n",
            "Batch 1600, D Loss: 0.6379644870758057, G Loss: 0.6959909796714783\n",
            "Batch 1650, D Loss: 0.6573883295059204, G Loss: 0.7770096063613892\n",
            "Batch 1700, D Loss: 0.6765923500061035, G Loss: 0.8495653867721558\n",
            "Batch 1750, D Loss: 0.6073087453842163, G Loss: 0.7156526446342468\n",
            "Batch 1800, D Loss: 0.6251569390296936, G Loss: 0.9334819316864014\n",
            "Batch 1850, D Loss: 0.648640513420105, G Loss: 0.8896064758300781\n",
            "Epoch 2, D Loss: 0.6618245840072632, G Loss: 0.6723328828811646\n",
            "Starting Epoch 3\n",
            "Batch 0, D Loss: 0.6540732979774475, G Loss: 0.7975237369537354\n",
            "Batch 50, D Loss: 0.6194100379943848, G Loss: 0.8235046863555908\n",
            "Batch 100, D Loss: 0.6774023771286011, G Loss: 0.8130441904067993\n",
            "Batch 150, D Loss: 0.6889931559562683, G Loss: 0.7717446088790894\n",
            "Batch 200, D Loss: 0.6090986728668213, G Loss: 0.7269876003265381\n",
            "Batch 250, D Loss: 0.6206648349761963, G Loss: 0.7279022336006165\n",
            "Batch 300, D Loss: 0.6334788203239441, G Loss: 0.8166066408157349\n",
            "Batch 350, D Loss: 0.667938232421875, G Loss: 0.8527991771697998\n",
            "Batch 400, D Loss: 0.6377039551734924, G Loss: 0.8020310997962952\n",
            "Batch 450, D Loss: 0.6859658360481262, G Loss: 0.652198314666748\n",
            "Batch 500, D Loss: 0.5563130378723145, G Loss: 0.7294197082519531\n",
            "Batch 550, D Loss: 0.6410606503486633, G Loss: 0.6666682362556458\n",
            "Batch 600, D Loss: 0.6384145021438599, G Loss: 0.8049540519714355\n",
            "Batch 650, D Loss: 0.6307823657989502, G Loss: 0.7035732269287109\n",
            "Batch 700, D Loss: 0.6362237930297852, G Loss: 0.7417997121810913\n",
            "Batch 750, D Loss: 0.6373476386070251, G Loss: 0.6707422137260437\n",
            "Batch 800, D Loss: 0.674176812171936, G Loss: 0.7422704696655273\n",
            "Batch 850, D Loss: 0.6294283866882324, G Loss: 0.7807555198669434\n",
            "Batch 900, D Loss: 0.6522580981254578, G Loss: 0.7882234454154968\n",
            "Batch 950, D Loss: 0.6529017686843872, G Loss: 0.876331627368927\n",
            "Batch 1000, D Loss: 0.6806982755661011, G Loss: 0.7001261711120605\n",
            "Batch 1050, D Loss: 0.684830904006958, G Loss: 0.7458401918411255\n",
            "Batch 1100, D Loss: 0.7057576775550842, G Loss: 0.7538762092590332\n",
            "Batch 1150, D Loss: 0.7051161527633667, G Loss: 0.7049528360366821\n",
            "Batch 1200, D Loss: 0.6415054798126221, G Loss: 0.8163225650787354\n",
            "Batch 1250, D Loss: 0.6647157073020935, G Loss: 0.8365391492843628\n",
            "Batch 1300, D Loss: 0.665569543838501, G Loss: 0.7531477212905884\n",
            "Batch 1350, D Loss: 0.6531074047088623, G Loss: 0.7204962372779846\n",
            "Batch 1400, D Loss: 0.663973331451416, G Loss: 0.8573434352874756\n",
            "Batch 1450, D Loss: 0.683965802192688, G Loss: 1.0408127307891846\n",
            "Batch 1500, D Loss: 0.7246610522270203, G Loss: 0.7251558303833008\n",
            "Batch 1550, D Loss: 0.6405501961708069, G Loss: 0.8792371153831482\n",
            "Batch 1600, D Loss: 0.6137466430664062, G Loss: 0.7833815813064575\n",
            "Batch 1650, D Loss: 0.6169435977935791, G Loss: 0.7348591089248657\n",
            "Batch 1700, D Loss: 0.626703679561615, G Loss: 0.7281630039215088\n",
            "Batch 1750, D Loss: 0.6298521757125854, G Loss: 0.6784014701843262\n",
            "Batch 1800, D Loss: 0.6654669046401978, G Loss: 0.7378734350204468\n",
            "Batch 1850, D Loss: 0.653929591178894, G Loss: 0.6861177682876587\n",
            "Epoch 3, D Loss: 0.6711434721946716, G Loss: 0.6932764053344727\n",
            "Starting Epoch 4\n",
            "Batch 0, D Loss: 0.6947357654571533, G Loss: 0.6657481789588928\n",
            "Batch 50, D Loss: 0.6547131538391113, G Loss: 0.6912988424301147\n",
            "Batch 100, D Loss: 0.696863055229187, G Loss: 0.7348275184631348\n",
            "Batch 150, D Loss: 0.6834450364112854, G Loss: 0.6880005598068237\n",
            "Batch 200, D Loss: 0.6129393577575684, G Loss: 0.6791432499885559\n",
            "Batch 250, D Loss: 0.6970008611679077, G Loss: 0.6289787292480469\n",
            "Batch 300, D Loss: 0.6189718842506409, G Loss: 0.7145955562591553\n",
            "Batch 350, D Loss: 0.6375459432601929, G Loss: 0.7707521915435791\n",
            "Batch 400, D Loss: 0.722939133644104, G Loss: 0.8729429244995117\n",
            "Batch 450, D Loss: 0.6725926399230957, G Loss: 0.8804277181625366\n",
            "Batch 500, D Loss: 0.690340518951416, G Loss: 0.6939113140106201\n",
            "Batch 550, D Loss: 0.7029992341995239, G Loss: 0.7884796857833862\n",
            "Batch 600, D Loss: 0.646439790725708, G Loss: 0.7871119976043701\n",
            "Batch 650, D Loss: 0.6343344449996948, G Loss: 0.6576687693595886\n",
            "Batch 700, D Loss: 0.662713348865509, G Loss: 0.8171277046203613\n",
            "Batch 750, D Loss: 0.6652681827545166, G Loss: 0.687241792678833\n",
            "Batch 800, D Loss: 0.6482864618301392, G Loss: 0.7268529534339905\n",
            "Batch 850, D Loss: 0.643004298210144, G Loss: 0.6879900693893433\n",
            "Batch 900, D Loss: 0.6483754515647888, G Loss: 0.7313755750656128\n",
            "Batch 950, D Loss: 0.6528234481811523, G Loss: 0.7451490759849548\n",
            "Batch 1000, D Loss: 0.6518260836601257, G Loss: 0.8250872492790222\n",
            "Batch 1050, D Loss: 0.6401649713516235, G Loss: 0.7308255434036255\n",
            "Batch 1100, D Loss: 0.6738420724868774, G Loss: 0.6984054446220398\n",
            "Batch 1150, D Loss: 0.7009888291358948, G Loss: 0.8170474767684937\n",
            "Batch 1200, D Loss: 0.6443452835083008, G Loss: 0.800222635269165\n",
            "Batch 1250, D Loss: 0.6230837106704712, G Loss: 0.8521404266357422\n",
            "Batch 1300, D Loss: 0.7371059060096741, G Loss: 0.7514885067939758\n",
            "Batch 1350, D Loss: 0.6595375537872314, G Loss: 0.7563544511795044\n",
            "Batch 1400, D Loss: 0.6990830898284912, G Loss: 0.7893998622894287\n",
            "Batch 1450, D Loss: 0.6437504887580872, G Loss: 0.7861504554748535\n",
            "Batch 1500, D Loss: 0.6370824575424194, G Loss: 0.8404465913772583\n",
            "Batch 1550, D Loss: 0.696976900100708, G Loss: 0.7190852165222168\n",
            "Batch 1600, D Loss: 0.6983223557472229, G Loss: 0.7167379856109619\n",
            "Batch 1650, D Loss: 0.6525304913520813, G Loss: 0.7665494680404663\n",
            "Batch 1700, D Loss: 0.6408475041389465, G Loss: 0.860745906829834\n",
            "Batch 1750, D Loss: 0.6922537088394165, G Loss: 0.7568163871765137\n",
            "Batch 1800, D Loss: 0.6864462494850159, G Loss: 0.6879124045372009\n",
            "Batch 1850, D Loss: 0.6478028893470764, G Loss: 0.7944498658180237\n",
            "Epoch 4, D Loss: 0.6511921882629395, G Loss: 0.7746509313583374\n",
            "Starting Epoch 5\n",
            "Batch 0, D Loss: 0.6585912704467773, G Loss: 0.7765617370605469\n",
            "Batch 50, D Loss: 0.6785907745361328, G Loss: 0.7905642986297607\n",
            "Batch 100, D Loss: 0.6171435117721558, G Loss: 0.7680914998054504\n",
            "Batch 150, D Loss: 0.6510276794433594, G Loss: 0.7446670532226562\n",
            "Batch 200, D Loss: 0.6745837926864624, G Loss: 0.7419899702072144\n",
            "Batch 250, D Loss: 0.6236258149147034, G Loss: 0.7652729153633118\n",
            "Batch 300, D Loss: 0.7052436470985413, G Loss: 0.7530553340911865\n",
            "Batch 350, D Loss: 0.6808269023895264, G Loss: 0.7362865209579468\n",
            "Batch 400, D Loss: 0.687562108039856, G Loss: 0.7272034883499146\n",
            "Batch 450, D Loss: 0.6455603837966919, G Loss: 0.6909795999526978\n",
            "Batch 500, D Loss: 0.641983151435852, G Loss: 0.6257565021514893\n",
            "Batch 550, D Loss: 0.6689468622207642, G Loss: 0.7325939536094666\n",
            "Batch 600, D Loss: 0.6792560815811157, G Loss: 0.7016918659210205\n",
            "Batch 650, D Loss: 0.677639365196228, G Loss: 0.8360850811004639\n",
            "Batch 700, D Loss: 0.693867564201355, G Loss: 0.7390632033348083\n",
            "Batch 750, D Loss: 0.6555718183517456, G Loss: 0.7032057046890259\n",
            "Batch 800, D Loss: 0.6831254959106445, G Loss: 0.7147971391677856\n",
            "Batch 850, D Loss: 0.6608563661575317, G Loss: 0.7549307942390442\n",
            "Batch 900, D Loss: 0.6428964734077454, G Loss: 0.7037193775177002\n",
            "Batch 950, D Loss: 0.6331126093864441, G Loss: 0.7163581252098083\n",
            "Batch 1000, D Loss: 0.6627768278121948, G Loss: 0.8108693361282349\n",
            "Batch 1050, D Loss: 0.6477086544036865, G Loss: 0.8029261827468872\n",
            "Batch 1100, D Loss: 0.6712968349456787, G Loss: 0.6969648599624634\n",
            "Batch 1150, D Loss: 0.6244299411773682, G Loss: 0.6702584624290466\n",
            "Batch 1200, D Loss: 0.6416043639183044, G Loss: 0.7282572984695435\n",
            "Batch 1250, D Loss: 0.7182489633560181, G Loss: 0.70441734790802\n",
            "Batch 1300, D Loss: 0.6934096813201904, G Loss: 0.7459707856178284\n",
            "Batch 1350, D Loss: 0.592729389667511, G Loss: 0.7638027667999268\n",
            "Batch 1400, D Loss: 0.6976425647735596, G Loss: 0.7595065832138062\n",
            "Batch 1450, D Loss: 0.6566070914268494, G Loss: 0.7147248983383179\n",
            "Batch 1500, D Loss: 0.652509331703186, G Loss: 0.6822707653045654\n",
            "Batch 1550, D Loss: 0.6230527758598328, G Loss: 0.6955937743186951\n",
            "Batch 1600, D Loss: 0.6511255502700806, G Loss: 0.6369460821151733\n",
            "Batch 1650, D Loss: 0.7148641347885132, G Loss: 0.7154764533042908\n",
            "Batch 1700, D Loss: 0.7044486999511719, G Loss: 0.7464380264282227\n",
            "Batch 1750, D Loss: 0.6323952674865723, G Loss: 0.7481629848480225\n",
            "Batch 1800, D Loss: 0.6723626255989075, G Loss: 0.7773168087005615\n",
            "Batch 1850, D Loss: 0.6972882747650146, G Loss: 0.7022310495376587\n",
            "Epoch 5, D Loss: 0.6470291614532471, G Loss: 0.6947479248046875\n",
            "Starting Epoch 6\n",
            "Batch 0, D Loss: 0.6574805974960327, G Loss: 0.8288874626159668\n",
            "Batch 50, D Loss: 0.6765375137329102, G Loss: 0.7171368598937988\n",
            "Batch 100, D Loss: 0.695614218711853, G Loss: 0.6872909069061279\n",
            "Batch 150, D Loss: 0.658668577671051, G Loss: 0.699717104434967\n",
            "Batch 200, D Loss: 0.644817590713501, G Loss: 0.801365315914154\n",
            "Batch 250, D Loss: 0.6981242895126343, G Loss: 0.7610949277877808\n",
            "Batch 300, D Loss: 0.7150025367736816, G Loss: 0.7127749919891357\n",
            "Batch 350, D Loss: 0.6424760222434998, G Loss: 0.7596824765205383\n",
            "Batch 400, D Loss: 0.689453661441803, G Loss: 0.761023759841919\n",
            "Batch 450, D Loss: 0.6431814432144165, G Loss: 0.7591774463653564\n",
            "Batch 500, D Loss: 0.6242548227310181, G Loss: 0.7870510816574097\n",
            "Batch 550, D Loss: 0.7037752866744995, G Loss: 0.7640911340713501\n",
            "Batch 600, D Loss: 0.6317489147186279, G Loss: 0.8392437100410461\n",
            "Batch 650, D Loss: 0.6884222626686096, G Loss: 0.7118583917617798\n",
            "Batch 700, D Loss: 0.6536109447479248, G Loss: 0.8607994318008423\n",
            "Batch 750, D Loss: 0.6751518845558167, G Loss: 0.6744419932365417\n",
            "Batch 800, D Loss: 0.6617940664291382, G Loss: 0.7475283741950989\n",
            "Batch 850, D Loss: 0.6101062297821045, G Loss: 0.7983764410018921\n",
            "Batch 900, D Loss: 0.6441425085067749, G Loss: 0.5846731066703796\n",
            "Batch 950, D Loss: 0.6240816116333008, G Loss: 0.8234339952468872\n",
            "Batch 1000, D Loss: 0.6341350674629211, G Loss: 0.7006281614303589\n",
            "Batch 1050, D Loss: 0.6438455581665039, G Loss: 0.8254709243774414\n",
            "Batch 1100, D Loss: 0.6731239557266235, G Loss: 0.7202664613723755\n",
            "Batch 1150, D Loss: 0.675175130367279, G Loss: 0.7530199289321899\n",
            "Batch 1200, D Loss: 0.6841629147529602, G Loss: 0.8085024356842041\n",
            "Batch 1250, D Loss: 0.6661264896392822, G Loss: 0.7499109506607056\n",
            "Batch 1300, D Loss: 0.6661543846130371, G Loss: 0.6848244667053223\n",
            "Batch 1350, D Loss: 0.6592104434967041, G Loss: 0.7557066679000854\n",
            "Batch 1400, D Loss: 0.6370205283164978, G Loss: 0.6103852987289429\n",
            "Batch 1450, D Loss: 0.630490243434906, G Loss: 0.8578668236732483\n",
            "Batch 1500, D Loss: 0.6130378246307373, G Loss: 0.8441869020462036\n",
            "Batch 1550, D Loss: 0.6735200881958008, G Loss: 0.7043296098709106\n",
            "Batch 1600, D Loss: 0.6386818885803223, G Loss: 0.7540947198867798\n",
            "Batch 1650, D Loss: 0.6282466650009155, G Loss: 0.7781427502632141\n",
            "Batch 1700, D Loss: 0.6636173129081726, G Loss: 0.7690999507904053\n",
            "Batch 1750, D Loss: 0.6560444831848145, G Loss: 0.7858180403709412\n",
            "Batch 1800, D Loss: 0.6391042470932007, G Loss: 0.8290859460830688\n",
            "Batch 1850, D Loss: 0.6685304641723633, G Loss: 0.6991921663284302\n",
            "Epoch 6, D Loss: 0.6791762709617615, G Loss: 0.7001277208328247\n",
            "Starting Epoch 7\n",
            "Batch 0, D Loss: 0.6387706995010376, G Loss: 0.8563941717147827\n",
            "Batch 50, D Loss: 0.6998159885406494, G Loss: 0.764174222946167\n",
            "Batch 100, D Loss: 0.6836174726486206, G Loss: 0.7547537088394165\n",
            "Batch 150, D Loss: 0.6621880531311035, G Loss: 0.7405726313591003\n",
            "Batch 200, D Loss: 0.6677535772323608, G Loss: 0.770228385925293\n",
            "Batch 250, D Loss: 0.6446474194526672, G Loss: 0.787743330001831\n",
            "Batch 300, D Loss: 0.6738442778587341, G Loss: 0.6625980138778687\n",
            "Batch 350, D Loss: 0.6781984567642212, G Loss: 0.7301949262619019\n",
            "Batch 400, D Loss: 0.6374380588531494, G Loss: 0.782694935798645\n",
            "Batch 450, D Loss: 0.6625933647155762, G Loss: 0.716833233833313\n",
            "Batch 500, D Loss: 0.6531827449798584, G Loss: 0.74109947681427\n",
            "Batch 550, D Loss: 0.6793899536132812, G Loss: 0.7560989260673523\n",
            "Batch 600, D Loss: 0.6829006671905518, G Loss: 0.7822116613388062\n",
            "Batch 650, D Loss: 0.6730936169624329, G Loss: 0.8553383350372314\n",
            "Batch 700, D Loss: 0.6673662662506104, G Loss: 0.8099194765090942\n",
            "Batch 750, D Loss: 0.6695263981819153, G Loss: 0.6893215179443359\n",
            "Batch 800, D Loss: 0.6510442495346069, G Loss: 0.7763060331344604\n",
            "Batch 850, D Loss: 0.6814153790473938, G Loss: 0.7726016044616699\n",
            "Batch 900, D Loss: 0.6285974383354187, G Loss: 0.8044922351837158\n",
            "Batch 950, D Loss: 0.6450220942497253, G Loss: 0.7359604835510254\n",
            "Batch 1000, D Loss: 0.6600644588470459, G Loss: 0.7398081421852112\n",
            "Batch 1050, D Loss: 0.7066013813018799, G Loss: 0.7757928371429443\n",
            "Batch 1100, D Loss: 0.6212141513824463, G Loss: 0.7451842427253723\n",
            "Batch 1150, D Loss: 0.5810301303863525, G Loss: 0.6619300842285156\n",
            "Batch 1200, D Loss: 0.6376229524612427, G Loss: 0.8040502071380615\n",
            "Batch 1250, D Loss: 0.6272789239883423, G Loss: 0.7984445691108704\n",
            "Batch 1300, D Loss: 0.7394218444824219, G Loss: 0.7457494735717773\n",
            "Batch 1350, D Loss: 0.6147037744522095, G Loss: 0.6740679144859314\n",
            "Batch 1400, D Loss: 0.6700072288513184, G Loss: 0.8185022473335266\n",
            "Batch 1450, D Loss: 0.6956721544265747, G Loss: 0.8463379144668579\n",
            "Batch 1500, D Loss: 0.6908802390098572, G Loss: 0.7788101434707642\n",
            "Batch 1550, D Loss: 0.6405957937240601, G Loss: 0.7195110321044922\n",
            "Batch 1600, D Loss: 0.7065898180007935, G Loss: 0.6568847894668579\n",
            "Batch 1650, D Loss: 0.6966774463653564, G Loss: 0.774187445640564\n",
            "Batch 1700, D Loss: 0.6440620422363281, G Loss: 0.7790753841400146\n",
            "Batch 1750, D Loss: 0.6269954442977905, G Loss: 0.7275386452674866\n",
            "Batch 1800, D Loss: 0.6163314580917358, G Loss: 0.7919784784317017\n",
            "Batch 1850, D Loss: 0.6563554406166077, G Loss: 0.8971918821334839\n",
            "Epoch 7, D Loss: 0.6819372177124023, G Loss: 0.7929956316947937\n",
            "Starting Epoch 8\n",
            "Batch 0, D Loss: 0.6650631427764893, G Loss: 0.8877241611480713\n",
            "Batch 50, D Loss: 0.629238486289978, G Loss: 0.6713652610778809\n",
            "Batch 100, D Loss: 0.6434913873672485, G Loss: 0.8336042165756226\n",
            "Batch 150, D Loss: 0.6327546238899231, G Loss: 0.7958043813705444\n",
            "Batch 200, D Loss: 0.6395276784896851, G Loss: 0.7363913059234619\n",
            "Batch 250, D Loss: 0.6578128337860107, G Loss: 0.8104901313781738\n",
            "Batch 300, D Loss: 0.673172652721405, G Loss: 0.655614972114563\n",
            "Batch 350, D Loss: 0.6639062166213989, G Loss: 0.7013165354728699\n",
            "Batch 400, D Loss: 0.668954074382782, G Loss: 0.7065833210945129\n",
            "Batch 450, D Loss: 0.6825509667396545, G Loss: 0.6412826776504517\n",
            "Batch 500, D Loss: 0.621638298034668, G Loss: 0.712813138961792\n",
            "Batch 550, D Loss: 0.6714550256729126, G Loss: 0.7120169401168823\n",
            "Batch 600, D Loss: 0.6300351023674011, G Loss: 0.7011547088623047\n",
            "Batch 650, D Loss: 0.6886515617370605, G Loss: 0.7446777820587158\n",
            "Batch 700, D Loss: 0.6605606079101562, G Loss: 0.8592562675476074\n",
            "Batch 750, D Loss: 0.6597108244895935, G Loss: 0.6816316843032837\n",
            "Batch 800, D Loss: 0.6765317320823669, G Loss: 0.8202148079872131\n",
            "Batch 850, D Loss: 0.6653706431388855, G Loss: 0.735781729221344\n",
            "Batch 900, D Loss: 0.6441371440887451, G Loss: 0.7721225023269653\n",
            "Batch 950, D Loss: 0.6632668972015381, G Loss: 0.7686462998390198\n",
            "Batch 1000, D Loss: 0.657363772392273, G Loss: 0.7488914728164673\n",
            "Batch 1050, D Loss: 0.6594493389129639, G Loss: 0.7466344833374023\n",
            "Batch 1100, D Loss: 0.6799440383911133, G Loss: 0.8681784868240356\n",
            "Batch 1150, D Loss: 0.6775117516517639, G Loss: 0.790765106678009\n",
            "Batch 1200, D Loss: 0.6703863143920898, G Loss: 0.832094669342041\n",
            "Batch 1250, D Loss: 0.6738521456718445, G Loss: 0.6739718317985535\n",
            "Batch 1300, D Loss: 0.6360554695129395, G Loss: 0.8259649276733398\n",
            "Batch 1350, D Loss: 0.6340702772140503, G Loss: 0.698954701423645\n",
            "Batch 1400, D Loss: 0.6580830216407776, G Loss: 0.7703126668930054\n",
            "Batch 1450, D Loss: 0.6814729571342468, G Loss: 0.7930971384048462\n",
            "Batch 1500, D Loss: 0.6635655164718628, G Loss: 0.8601954579353333\n",
            "Batch 1550, D Loss: 0.6614975333213806, G Loss: 0.7233840227127075\n",
            "Batch 1600, D Loss: 0.7179194688796997, G Loss: 0.7731480598449707\n",
            "Batch 1650, D Loss: 0.6668651700019836, G Loss: 0.7224398851394653\n",
            "Batch 1700, D Loss: 0.6850329637527466, G Loss: 0.7720452547073364\n",
            "Batch 1750, D Loss: 0.6306886672973633, G Loss: 0.6987775564193726\n",
            "Batch 1800, D Loss: 0.6149548292160034, G Loss: 0.7681556940078735\n",
            "Batch 1850, D Loss: 0.6518208384513855, G Loss: 0.7829809784889221\n",
            "Epoch 8, D Loss: 0.6785313487052917, G Loss: 0.8181799650192261\n",
            "Starting Epoch 9\n",
            "Batch 0, D Loss: 0.6680549383163452, G Loss: 0.8679736852645874\n",
            "Batch 50, D Loss: 0.6812931299209595, G Loss: 0.7242574691772461\n",
            "Batch 100, D Loss: 0.65544593334198, G Loss: 0.779251217842102\n",
            "Batch 150, D Loss: 0.6234002113342285, G Loss: 0.6816799640655518\n",
            "Batch 200, D Loss: 0.6680811047554016, G Loss: 0.7958985567092896\n",
            "Batch 250, D Loss: 0.6534599661827087, G Loss: 0.7255008220672607\n",
            "Batch 300, D Loss: 0.6456450819969177, G Loss: 0.7546862363815308\n",
            "Batch 350, D Loss: 0.6774702072143555, G Loss: 0.8439723253250122\n",
            "Batch 400, D Loss: 0.6979708075523376, G Loss: 0.8418352603912354\n",
            "Batch 450, D Loss: 0.6884232759475708, G Loss: 0.7834857106208801\n",
            "Batch 500, D Loss: 0.684245228767395, G Loss: 0.7446314096450806\n",
            "Batch 550, D Loss: 0.6171888709068298, G Loss: 0.6583614349365234\n",
            "Batch 600, D Loss: 0.6873810291290283, G Loss: 0.7795267701148987\n",
            "Batch 650, D Loss: 0.6587743759155273, G Loss: 0.8796838521957397\n",
            "Batch 700, D Loss: 0.6855113506317139, G Loss: 0.7543073892593384\n",
            "Batch 750, D Loss: 0.65848308801651, G Loss: 0.8139359951019287\n",
            "Batch 800, D Loss: 0.728760838508606, G Loss: 0.892971396446228\n",
            "Batch 850, D Loss: 0.6253231763839722, G Loss: 0.8171525597572327\n",
            "Batch 900, D Loss: 0.6495181322097778, G Loss: 0.8084504008293152\n",
            "Batch 950, D Loss: 0.648377537727356, G Loss: 0.667726993560791\n",
            "Batch 1000, D Loss: 0.629562497138977, G Loss: 0.719106912612915\n",
            "Batch 1050, D Loss: 0.6487150192260742, G Loss: 0.7902345657348633\n",
            "Batch 1100, D Loss: 0.6656131148338318, G Loss: 0.7771779298782349\n",
            "Batch 1150, D Loss: 0.674446702003479, G Loss: 0.6821492910385132\n",
            "Batch 1200, D Loss: 0.6495248079299927, G Loss: 0.7317609786987305\n",
            "Batch 1250, D Loss: 0.6904653310775757, G Loss: 0.6873050332069397\n",
            "Batch 1300, D Loss: 0.6531316041946411, G Loss: 0.761274516582489\n",
            "Batch 1350, D Loss: 0.6204293370246887, G Loss: 0.8949728012084961\n",
            "Batch 1400, D Loss: 0.6307880878448486, G Loss: 0.8410311937332153\n",
            "Batch 1450, D Loss: 0.7325037717819214, G Loss: 0.7993273138999939\n",
            "Batch 1500, D Loss: 0.652004063129425, G Loss: 0.814058244228363\n",
            "Batch 1550, D Loss: 0.6355633735656738, G Loss: 0.765238881111145\n",
            "Batch 1600, D Loss: 0.6354816555976868, G Loss: 0.7177319526672363\n",
            "Batch 1650, D Loss: 0.7375243306159973, G Loss: 0.8548543453216553\n",
            "Batch 1700, D Loss: 0.6470829248428345, G Loss: 0.7154033184051514\n",
            "Batch 1750, D Loss: 0.685352087020874, G Loss: 0.7549794316291809\n",
            "Batch 1800, D Loss: 0.6589434146881104, G Loss: 0.7838413715362549\n",
            "Batch 1850, D Loss: 0.6462955474853516, G Loss: 0.9068148136138916\n",
            "Epoch 9, D Loss: 0.6710482239723206, G Loss: 0.7861864566802979\n",
            "Starting Epoch 10\n",
            "Batch 0, D Loss: 0.7171609401702881, G Loss: 0.829298734664917\n",
            "Batch 50, D Loss: 0.6621600389480591, G Loss: 0.8823938369750977\n",
            "Batch 100, D Loss: 0.6576521396636963, G Loss: 0.7273820638656616\n",
            "Batch 150, D Loss: 0.6434903144836426, G Loss: 0.806517481803894\n",
            "Batch 200, D Loss: 0.641848623752594, G Loss: 0.6925716996192932\n",
            "Batch 250, D Loss: 0.6570501923561096, G Loss: 0.7489941120147705\n",
            "Batch 300, D Loss: 0.6854085922241211, G Loss: 0.6658514738082886\n",
            "Batch 350, D Loss: 0.6446419358253479, G Loss: 0.7224674820899963\n",
            "Batch 400, D Loss: 0.6499910354614258, G Loss: 0.9641002416610718\n",
            "Batch 450, D Loss: 0.6624552607536316, G Loss: 0.8669483661651611\n",
            "Batch 500, D Loss: 0.6696990132331848, G Loss: 0.7211399078369141\n",
            "Batch 550, D Loss: 0.6426935195922852, G Loss: 0.7579212188720703\n",
            "Batch 600, D Loss: 0.6803325414657593, G Loss: 0.7995635271072388\n",
            "Batch 650, D Loss: 0.5750068426132202, G Loss: 0.7124142050743103\n",
            "Batch 700, D Loss: 0.6318291425704956, G Loss: 0.8102171421051025\n",
            "Batch 750, D Loss: 0.6617296934127808, G Loss: 0.8426690101623535\n",
            "Batch 800, D Loss: 0.6427493691444397, G Loss: 0.7817312479019165\n",
            "Batch 850, D Loss: 0.6474834084510803, G Loss: 0.7316356897354126\n",
            "Batch 900, D Loss: 0.6231950521469116, G Loss: 0.794154703617096\n",
            "Batch 950, D Loss: 0.6918584108352661, G Loss: 0.725334644317627\n",
            "Batch 1000, D Loss: 0.6830053329467773, G Loss: 0.6522270441055298\n",
            "Batch 1050, D Loss: 0.6018556356430054, G Loss: 0.7284951210021973\n",
            "Batch 1100, D Loss: 0.6428418159484863, G Loss: 0.7824050188064575\n",
            "Batch 1150, D Loss: 0.6415847539901733, G Loss: 0.7771684527397156\n",
            "Batch 1200, D Loss: 0.6633872985839844, G Loss: 0.8762613534927368\n",
            "Batch 1250, D Loss: 0.6496734619140625, G Loss: 0.7609027624130249\n",
            "Batch 1300, D Loss: 0.6617045402526855, G Loss: 0.8543727993965149\n",
            "Batch 1350, D Loss: 0.6319370269775391, G Loss: 0.8171826601028442\n",
            "Batch 1400, D Loss: 0.6393091678619385, G Loss: 0.9247856140136719\n",
            "Batch 1450, D Loss: 0.6281593441963196, G Loss: 0.8032229542732239\n",
            "Batch 1500, D Loss: 0.6219005584716797, G Loss: 0.7310270667076111\n",
            "Batch 1550, D Loss: 0.632301390171051, G Loss: 0.8538545370101929\n",
            "Batch 1600, D Loss: 0.7390497922897339, G Loss: 0.8105143308639526\n",
            "Batch 1650, D Loss: 0.6449828147888184, G Loss: 0.752565324306488\n",
            "Batch 1700, D Loss: 0.614484965801239, G Loss: 0.8232669830322266\n",
            "Batch 1750, D Loss: 0.6537412405014038, G Loss: 0.8044486045837402\n",
            "Batch 1800, D Loss: 0.6481447219848633, G Loss: 0.7618976831436157\n",
            "Batch 1850, D Loss: 0.6328225135803223, G Loss: 0.8903271555900574\n",
            "Epoch 10, D Loss: 0.5978809595108032, G Loss: 0.8011081218719482\n",
            "Starting Epoch 11\n",
            "Batch 0, D Loss: 0.6331108808517456, G Loss: 0.7514036297798157\n",
            "Batch 50, D Loss: 0.6177005767822266, G Loss: 0.7358071804046631\n",
            "Batch 100, D Loss: 0.6780143976211548, G Loss: 0.6927542686462402\n",
            "Batch 150, D Loss: 0.6211264133453369, G Loss: 0.8168129920959473\n",
            "Batch 200, D Loss: 0.6496175527572632, G Loss: 0.8973082900047302\n",
            "Batch 250, D Loss: 0.6709929704666138, G Loss: 0.8023655414581299\n",
            "Batch 300, D Loss: 0.6664199233055115, G Loss: 0.698336124420166\n",
            "Batch 350, D Loss: 0.737883448600769, G Loss: 0.8012855052947998\n",
            "Batch 400, D Loss: 0.6399896144866943, G Loss: 0.7899349927902222\n",
            "Batch 450, D Loss: 0.638799250125885, G Loss: 0.9007267355918884\n",
            "Batch 500, D Loss: 0.6517484188079834, G Loss: 0.6700018048286438\n",
            "Batch 550, D Loss: 0.6386657953262329, G Loss: 0.7434147596359253\n",
            "Batch 600, D Loss: 0.6488568782806396, G Loss: 0.7766776084899902\n",
            "Batch 650, D Loss: 0.6630964279174805, G Loss: 0.8036980628967285\n",
            "Batch 700, D Loss: 0.6314122676849365, G Loss: 0.8083921074867249\n",
            "Batch 750, D Loss: 0.6389386653900146, G Loss: 0.7103530168533325\n",
            "Batch 800, D Loss: 0.6056901216506958, G Loss: 0.7641996741294861\n",
            "Batch 850, D Loss: 0.6456156969070435, G Loss: 0.7484720945358276\n",
            "Batch 900, D Loss: 0.6873832941055298, G Loss: 0.9333527088165283\n",
            "Batch 950, D Loss: 0.692673921585083, G Loss: 0.7660976052284241\n",
            "Batch 1000, D Loss: 0.6732338666915894, G Loss: 0.7764496803283691\n",
            "Batch 1050, D Loss: 0.6928550004959106, G Loss: 0.799511730670929\n",
            "Batch 1100, D Loss: 0.617896318435669, G Loss: 0.7992724180221558\n",
            "Batch 1150, D Loss: 0.6137530207633972, G Loss: 0.8283644914627075\n",
            "Batch 1200, D Loss: 0.6146112680435181, G Loss: 0.9087420701980591\n",
            "Batch 1250, D Loss: 0.6530998945236206, G Loss: 0.7971717119216919\n",
            "Batch 1300, D Loss: 0.6586679220199585, G Loss: 0.8382225632667542\n",
            "Batch 1350, D Loss: 0.6683336496353149, G Loss: 0.7620933055877686\n",
            "Batch 1400, D Loss: 0.6169915199279785, G Loss: 0.7873519659042358\n",
            "Batch 1450, D Loss: 0.6872537136077881, G Loss: 0.7370284795761108\n",
            "Batch 1500, D Loss: 0.619688093662262, G Loss: 0.8441334366798401\n",
            "Batch 1550, D Loss: 0.6609579920768738, G Loss: 0.790651798248291\n",
            "Batch 1600, D Loss: 0.6981094479560852, G Loss: 0.7935199737548828\n",
            "Batch 1650, D Loss: 0.6125848293304443, G Loss: 0.8146607875823975\n",
            "Batch 1700, D Loss: 0.6631104946136475, G Loss: 0.8059462308883667\n",
            "Batch 1750, D Loss: 0.663590669631958, G Loss: 0.7242392301559448\n",
            "Batch 1800, D Loss: 0.5665597915649414, G Loss: 0.6915364861488342\n",
            "Batch 1850, D Loss: 0.6635506749153137, G Loss: 0.8888139128684998\n",
            "Epoch 11, D Loss: 0.6171997785568237, G Loss: 0.8114752769470215\n",
            "Starting Epoch 12\n",
            "Batch 0, D Loss: 0.6944118738174438, G Loss: 0.7334609031677246\n",
            "Batch 50, D Loss: 0.6418935060501099, G Loss: 0.7908320426940918\n",
            "Batch 100, D Loss: 0.6372374296188354, G Loss: 0.8504611253738403\n",
            "Batch 150, D Loss: 0.6581140756607056, G Loss: 0.8038163781166077\n",
            "Batch 200, D Loss: 0.6406938433647156, G Loss: 0.7422130703926086\n",
            "Batch 250, D Loss: 0.6575965881347656, G Loss: 0.7506201267242432\n",
            "Batch 300, D Loss: 0.7090013027191162, G Loss: 0.8455542325973511\n",
            "Batch 350, D Loss: 0.6281745433807373, G Loss: 0.8590021133422852\n",
            "Batch 400, D Loss: 0.6076545715332031, G Loss: 0.7758077383041382\n",
            "Batch 450, D Loss: 0.6524380445480347, G Loss: 0.8699708580970764\n",
            "Batch 500, D Loss: 0.6508898735046387, G Loss: 0.8998767137527466\n",
            "Batch 550, D Loss: 0.6380833387374878, G Loss: 0.8283439874649048\n",
            "Batch 600, D Loss: 0.6610267162322998, G Loss: 0.790912389755249\n",
            "Batch 650, D Loss: 0.6483993530273438, G Loss: 0.6306400299072266\n",
            "Batch 700, D Loss: 0.611391544342041, G Loss: 0.9249100089073181\n",
            "Batch 750, D Loss: 0.6388063430786133, G Loss: 0.8440898656845093\n",
            "Batch 800, D Loss: 0.6622867584228516, G Loss: 0.7389572858810425\n",
            "Batch 850, D Loss: 0.6775742769241333, G Loss: 0.8991999626159668\n",
            "Batch 900, D Loss: 0.6447752714157104, G Loss: 0.7405914068222046\n",
            "Batch 950, D Loss: 0.6562254428863525, G Loss: 0.8258379101753235\n",
            "Batch 1000, D Loss: 0.6650656461715698, G Loss: 0.7821720838546753\n",
            "Batch 1050, D Loss: 0.6678940653800964, G Loss: 0.7078893184661865\n",
            "Batch 1100, D Loss: 0.6499158143997192, G Loss: 0.7937883138656616\n",
            "Batch 1150, D Loss: 0.6749037504196167, G Loss: 0.8150421380996704\n",
            "Batch 1200, D Loss: 0.7140594124794006, G Loss: 0.7972596883773804\n",
            "Batch 1250, D Loss: 0.6826639175415039, G Loss: 0.7298425436019897\n",
            "Batch 1300, D Loss: 0.6204068660736084, G Loss: 0.8234546780586243\n",
            "Batch 1350, D Loss: 0.6497277021408081, G Loss: 0.7789552807807922\n",
            "Batch 1400, D Loss: 0.6520976424217224, G Loss: 0.9336329698562622\n",
            "Batch 1450, D Loss: 0.6687173843383789, G Loss: 0.7619730830192566\n",
            "Batch 1500, D Loss: 0.6049305200576782, G Loss: 0.8893092274665833\n",
            "Batch 1550, D Loss: 0.7149831056594849, G Loss: 0.8425799608230591\n",
            "Batch 1600, D Loss: 0.6465270519256592, G Loss: 0.7526512145996094\n",
            "Batch 1650, D Loss: 0.6516779661178589, G Loss: 0.826363205909729\n",
            "Batch 1700, D Loss: 0.6492065191268921, G Loss: 0.8678800463676453\n",
            "Batch 1750, D Loss: 0.6614549160003662, G Loss: 0.7861098647117615\n",
            "Batch 1800, D Loss: 0.6275681257247925, G Loss: 0.7496783137321472\n",
            "Batch 1850, D Loss: 0.6177111864089966, G Loss: 0.7955960035324097\n",
            "Epoch 12, D Loss: 0.6490505337715149, G Loss: 0.7967963218688965\n",
            "Starting Epoch 13\n",
            "Batch 0, D Loss: 0.6395339965820312, G Loss: 0.7218862771987915\n",
            "Batch 50, D Loss: 0.669969379901886, G Loss: 0.8472206592559814\n",
            "Batch 100, D Loss: 0.6249622106552124, G Loss: 0.7191381454467773\n",
            "Batch 150, D Loss: 0.6155714988708496, G Loss: 0.8372886776924133\n",
            "Batch 200, D Loss: 0.5580812692642212, G Loss: 0.804410994052887\n",
            "Batch 250, D Loss: 0.6430671811103821, G Loss: 0.8985663056373596\n",
            "Batch 300, D Loss: 0.6650073528289795, G Loss: 0.7561821341514587\n",
            "Batch 350, D Loss: 0.6535351276397705, G Loss: 0.7754659652709961\n",
            "Batch 400, D Loss: 0.6564310193061829, G Loss: 0.9084900617599487\n",
            "Batch 450, D Loss: 0.638879656791687, G Loss: 0.8633971810340881\n",
            "Batch 500, D Loss: 0.6612132787704468, G Loss: 0.8211150169372559\n",
            "Batch 550, D Loss: 0.677139401435852, G Loss: 0.8198513984680176\n",
            "Batch 600, D Loss: 0.6679549217224121, G Loss: 0.7975285053253174\n",
            "Batch 650, D Loss: 0.6183938980102539, G Loss: 0.7788388729095459\n",
            "Batch 700, D Loss: 0.679300844669342, G Loss: 0.7671698331832886\n",
            "Batch 750, D Loss: 0.6881262063980103, G Loss: 0.8325656056404114\n",
            "Batch 800, D Loss: 0.6631482839584351, G Loss: 0.8183133602142334\n",
            "Batch 850, D Loss: 0.6676990985870361, G Loss: 0.7930898070335388\n",
            "Batch 900, D Loss: 0.6623014807701111, G Loss: 0.6962705850601196\n",
            "Batch 950, D Loss: 0.644658088684082, G Loss: 0.8186338543891907\n",
            "Batch 1000, D Loss: 0.6237857937812805, G Loss: 0.7675007581710815\n",
            "Batch 1050, D Loss: 0.6330013275146484, G Loss: 0.7624152898788452\n",
            "Batch 1100, D Loss: 0.6578510999679565, G Loss: 0.8433865904808044\n",
            "Batch 1150, D Loss: 0.6395548582077026, G Loss: 0.8160937428474426\n",
            "Batch 1200, D Loss: 0.6799706816673279, G Loss: 0.8421380519866943\n",
            "Batch 1250, D Loss: 0.7007606029510498, G Loss: 0.8075228333473206\n",
            "Batch 1300, D Loss: 0.6669580340385437, G Loss: 0.7672387361526489\n",
            "Batch 1350, D Loss: 0.6193052530288696, G Loss: 0.7856060266494751\n",
            "Batch 1400, D Loss: 0.6980307698249817, G Loss: 0.8423302173614502\n",
            "Batch 1450, D Loss: 0.6537050008773804, G Loss: 0.7900692224502563\n",
            "Batch 1500, D Loss: 0.6763455271720886, G Loss: 0.8130742311477661\n",
            "Batch 1550, D Loss: 0.6051895618438721, G Loss: 0.7274037599563599\n",
            "Batch 1600, D Loss: 0.6851881146430969, G Loss: 0.7987825274467468\n",
            "Batch 1650, D Loss: 0.639761209487915, G Loss: 0.7280141711235046\n",
            "Batch 1700, D Loss: 0.6912814378738403, G Loss: 0.8740878105163574\n",
            "Batch 1750, D Loss: 0.6808186769485474, G Loss: 0.719346284866333\n",
            "Batch 1800, D Loss: 0.6774423122406006, G Loss: 0.8549191355705261\n",
            "Batch 1850, D Loss: 0.625615119934082, G Loss: 0.8472325801849365\n",
            "Epoch 13, D Loss: 0.6830064058303833, G Loss: 0.8941431045532227\n",
            "Starting Epoch 14\n",
            "Batch 0, D Loss: 0.6677664518356323, G Loss: 0.8666777610778809\n",
            "Batch 50, D Loss: 0.6603255271911621, G Loss: 0.8225505948066711\n",
            "Batch 100, D Loss: 0.6795040965080261, G Loss: 0.8781087398529053\n",
            "Batch 150, D Loss: 0.6254492998123169, G Loss: 0.7381080389022827\n",
            "Batch 200, D Loss: 0.6334126591682434, G Loss: 0.8113406896591187\n",
            "Batch 250, D Loss: 0.6099911332130432, G Loss: 0.6895191669464111\n",
            "Batch 300, D Loss: 0.6682158708572388, G Loss: 0.8784459829330444\n",
            "Batch 350, D Loss: 0.6747094988822937, G Loss: 0.7470660209655762\n",
            "Batch 400, D Loss: 0.6310114860534668, G Loss: 0.7459356188774109\n",
            "Batch 450, D Loss: 0.7005982398986816, G Loss: 0.8417091369628906\n",
            "Batch 500, D Loss: 0.6794322729110718, G Loss: 0.8448793292045593\n",
            "Batch 550, D Loss: 0.6714638471603394, G Loss: 0.790739893913269\n",
            "Batch 600, D Loss: 0.6602478623390198, G Loss: 0.7979358434677124\n",
            "Batch 650, D Loss: 0.7013564109802246, G Loss: 0.8146138191223145\n",
            "Batch 700, D Loss: 0.5488174557685852, G Loss: 0.7883554697036743\n",
            "Batch 750, D Loss: 0.6696935892105103, G Loss: 0.7903769016265869\n",
            "Batch 800, D Loss: 0.652947723865509, G Loss: 0.8173990249633789\n",
            "Batch 850, D Loss: 0.6272269487380981, G Loss: 0.7513887286186218\n",
            "Batch 900, D Loss: 0.7030717134475708, G Loss: 0.8994934558868408\n",
            "Batch 950, D Loss: 0.6513200402259827, G Loss: 0.7883769273757935\n",
            "Batch 1000, D Loss: 0.669488787651062, G Loss: 0.7995539307594299\n",
            "Batch 1050, D Loss: 0.6738339066505432, G Loss: 0.7524799108505249\n",
            "Batch 1100, D Loss: 0.6579815149307251, G Loss: 0.882276713848114\n",
            "Batch 1150, D Loss: 0.7150936722755432, G Loss: 0.8083717226982117\n",
            "Batch 1200, D Loss: 0.6359657645225525, G Loss: 0.7342456579208374\n",
            "Batch 1250, D Loss: 0.6979581713676453, G Loss: 0.7353607416152954\n",
            "Batch 1300, D Loss: 0.6338173151016235, G Loss: 0.828593373298645\n",
            "Batch 1350, D Loss: 0.7223634719848633, G Loss: 0.8052442073822021\n",
            "Batch 1400, D Loss: 0.7371787428855896, G Loss: 0.8236056566238403\n",
            "Batch 1450, D Loss: 0.6416944265365601, G Loss: 0.743829607963562\n",
            "Batch 1500, D Loss: 0.6344953775405884, G Loss: 0.817638635635376\n",
            "Batch 1550, D Loss: 0.6794925928115845, G Loss: 0.8690113425254822\n",
            "Batch 1600, D Loss: 0.6080559492111206, G Loss: 0.7760056257247925\n",
            "Batch 1650, D Loss: 0.6439704895019531, G Loss: 0.7731814384460449\n",
            "Batch 1700, D Loss: 0.6898609399795532, G Loss: 0.7941768169403076\n",
            "Batch 1750, D Loss: 0.6779775619506836, G Loss: 0.7235512733459473\n",
            "Batch 1800, D Loss: 0.6396275758743286, G Loss: 0.8682732582092285\n",
            "Batch 1850, D Loss: 0.6458835601806641, G Loss: 0.7896748781204224\n",
            "Epoch 14, D Loss: 0.6411958932876587, G Loss: 0.8425991535186768\n",
            "Starting Epoch 15\n",
            "Batch 0, D Loss: 0.7306275367736816, G Loss: 0.7877054214477539\n",
            "Batch 50, D Loss: 0.6063379645347595, G Loss: 0.8199052810668945\n",
            "Batch 100, D Loss: 0.6617093086242676, G Loss: 0.8006789684295654\n",
            "Batch 150, D Loss: 0.6465509533882141, G Loss: 0.7566297054290771\n",
            "Batch 200, D Loss: 0.6505225896835327, G Loss: 0.7998998165130615\n",
            "Batch 250, D Loss: 0.7035164833068848, G Loss: 0.844150185585022\n",
            "Batch 300, D Loss: 0.6717413067817688, G Loss: 0.887789249420166\n",
            "Batch 350, D Loss: 0.6802407503128052, G Loss: 0.8020491600036621\n",
            "Batch 400, D Loss: 0.6471731662750244, G Loss: 0.8008817434310913\n",
            "Batch 450, D Loss: 0.6453804969787598, G Loss: 0.7554945349693298\n",
            "Batch 500, D Loss: 0.6272983551025391, G Loss: 0.7627232074737549\n",
            "Batch 550, D Loss: 0.6523516178131104, G Loss: 0.730053722858429\n",
            "Batch 600, D Loss: 0.6243300437927246, G Loss: 0.7435542941093445\n",
            "Batch 650, D Loss: 0.6228733062744141, G Loss: 0.8452182412147522\n",
            "Batch 700, D Loss: 0.6722263097763062, G Loss: 0.717343270778656\n",
            "Batch 750, D Loss: 0.6829964518547058, G Loss: 0.8630863428115845\n",
            "Batch 800, D Loss: 0.6416871547698975, G Loss: 0.793928861618042\n",
            "Batch 850, D Loss: 0.6637119054794312, G Loss: 0.7972794771194458\n",
            "Batch 900, D Loss: 0.6569428443908691, G Loss: 0.896788477897644\n",
            "Batch 950, D Loss: 0.6154848337173462, G Loss: 0.7568670511245728\n",
            "Batch 1000, D Loss: 0.6387454271316528, G Loss: 0.7941299676895142\n",
            "Batch 1050, D Loss: 0.6792545318603516, G Loss: 0.8806557655334473\n",
            "Batch 1100, D Loss: 0.6761022806167603, G Loss: 0.7231799364089966\n",
            "Batch 1150, D Loss: 0.6503470540046692, G Loss: 0.8646320700645447\n",
            "Batch 1200, D Loss: 0.6502467393875122, G Loss: 0.7912089228630066\n",
            "Batch 1250, D Loss: 0.6402292251586914, G Loss: 0.7753463983535767\n",
            "Batch 1300, D Loss: 0.6396414041519165, G Loss: 0.6953302025794983\n",
            "Batch 1350, D Loss: 0.6820480823516846, G Loss: 0.7779990434646606\n",
            "Batch 1400, D Loss: 0.6542549133300781, G Loss: 0.8505240678787231\n",
            "Batch 1450, D Loss: 0.6829657554626465, G Loss: 0.7615604400634766\n",
            "Batch 1500, D Loss: 0.6861084699630737, G Loss: 0.9067190289497375\n",
            "Batch 1550, D Loss: 0.6539903879165649, G Loss: 0.8919475078582764\n",
            "Batch 1600, D Loss: 0.6123349666595459, G Loss: 0.8440017700195312\n",
            "Batch 1650, D Loss: 0.6377060413360596, G Loss: 0.6936211585998535\n",
            "Batch 1700, D Loss: 0.659095823764801, G Loss: 0.7513949275016785\n",
            "Batch 1750, D Loss: 0.642146110534668, G Loss: 0.7998528480529785\n",
            "Batch 1800, D Loss: 0.6413833498954773, G Loss: 0.7033241987228394\n",
            "Batch 1850, D Loss: 0.6156664490699768, G Loss: 0.7679382562637329\n",
            "Epoch 15, D Loss: 0.6476972103118896, G Loss: 0.847793459892273\n",
            "Starting Epoch 16\n",
            "Batch 0, D Loss: 0.6328052282333374, G Loss: 0.7551661729812622\n",
            "Batch 50, D Loss: 0.6606951951980591, G Loss: 0.856660783290863\n",
            "Batch 100, D Loss: 0.6854842305183411, G Loss: 0.7702015042304993\n",
            "Batch 150, D Loss: 0.6588957905769348, G Loss: 0.768339991569519\n",
            "Batch 200, D Loss: 0.5896820425987244, G Loss: 0.8440364599227905\n",
            "Batch 250, D Loss: 0.6880584359169006, G Loss: 0.8431216478347778\n",
            "Batch 300, D Loss: 0.600830078125, G Loss: 0.7594213485717773\n",
            "Batch 350, D Loss: 0.6902511119842529, G Loss: 0.7956918478012085\n",
            "Batch 400, D Loss: 0.6364177465438843, G Loss: 0.8857302665710449\n",
            "Batch 450, D Loss: 0.671127438545227, G Loss: 0.8557623624801636\n",
            "Batch 500, D Loss: 0.6637328863143921, G Loss: 0.7705530524253845\n",
            "Batch 550, D Loss: 0.6505285501480103, G Loss: 0.7506105899810791\n",
            "Batch 600, D Loss: 0.6521896123886108, G Loss: 0.7993437051773071\n",
            "Batch 650, D Loss: 0.602969229221344, G Loss: 0.8247150182723999\n",
            "Batch 700, D Loss: 0.72120600938797, G Loss: 0.9122084975242615\n",
            "Batch 750, D Loss: 0.6088485717773438, G Loss: 0.8470664620399475\n",
            "Batch 800, D Loss: 0.6037609577178955, G Loss: 0.7415444850921631\n",
            "Batch 850, D Loss: 0.6635679602622986, G Loss: 0.8482455611228943\n",
            "Batch 900, D Loss: 0.6014853715896606, G Loss: 0.9530761241912842\n",
            "Batch 950, D Loss: 0.6437985897064209, G Loss: 0.8261442184448242\n",
            "Batch 1000, D Loss: 0.6555800437927246, G Loss: 0.7653344869613647\n",
            "Batch 1050, D Loss: 0.6903066635131836, G Loss: 0.819780707359314\n",
            "Batch 1100, D Loss: 0.6719614267349243, G Loss: 0.7885616421699524\n",
            "Batch 1150, D Loss: 0.6492986679077148, G Loss: 0.8502773642539978\n",
            "Batch 1200, D Loss: 0.6498645544052124, G Loss: 0.8143051862716675\n",
            "Batch 1250, D Loss: 0.6311465501785278, G Loss: 0.7965944409370422\n",
            "Batch 1300, D Loss: 0.6799865365028381, G Loss: 0.7638422250747681\n",
            "Batch 1350, D Loss: 0.655535101890564, G Loss: 0.7891354560852051\n",
            "Batch 1400, D Loss: 0.6687107086181641, G Loss: 0.7762953042984009\n",
            "Batch 1450, D Loss: 0.618067741394043, G Loss: 0.7382852435112\n",
            "Batch 1500, D Loss: 0.6606076955795288, G Loss: 0.818587064743042\n",
            "Batch 1550, D Loss: 0.6828375458717346, G Loss: 0.8236303329467773\n",
            "Batch 1600, D Loss: 0.6851009130477905, G Loss: 0.7762382626533508\n",
            "Batch 1650, D Loss: 0.6501588821411133, G Loss: 0.8596909642219543\n",
            "Batch 1700, D Loss: 0.6309720873832703, G Loss: 0.8427599668502808\n",
            "Batch 1750, D Loss: 0.7267656922340393, G Loss: 0.8137714266777039\n",
            "Batch 1800, D Loss: 0.753427267074585, G Loss: 0.7339725494384766\n",
            "Batch 1850, D Loss: 0.6357055902481079, G Loss: 0.8351358771324158\n",
            "Epoch 16, D Loss: 0.6526924967765808, G Loss: 0.8444026112556458\n",
            "Starting Epoch 17\n",
            "Batch 0, D Loss: 0.627893328666687, G Loss: 0.7621064186096191\n",
            "Batch 50, D Loss: 0.5983021259307861, G Loss: 0.7861096858978271\n",
            "Batch 100, D Loss: 0.6472820043563843, G Loss: 0.7615545988082886\n",
            "Batch 150, D Loss: 0.6549065113067627, G Loss: 0.8669135570526123\n",
            "Batch 200, D Loss: 0.6473597288131714, G Loss: 0.7483649849891663\n",
            "Batch 250, D Loss: 0.6152186989784241, G Loss: 0.8739396333694458\n",
            "Batch 300, D Loss: 0.6264930367469788, G Loss: 0.8165491819381714\n",
            "Batch 350, D Loss: 0.6872081756591797, G Loss: 0.819916844367981\n",
            "Batch 400, D Loss: 0.638016939163208, G Loss: 0.8456438779830933\n",
            "Batch 450, D Loss: 0.6589762568473816, G Loss: 0.7735792994499207\n",
            "Batch 500, D Loss: 0.6467286348342896, G Loss: 0.8072021007537842\n",
            "Batch 550, D Loss: 0.6029502153396606, G Loss: 0.8691823482513428\n",
            "Batch 600, D Loss: 0.5787316560745239, G Loss: 0.8972563743591309\n",
            "Batch 650, D Loss: 0.6910159587860107, G Loss: 0.900445282459259\n",
            "Batch 700, D Loss: 0.6337566375732422, G Loss: 0.8105103969573975\n",
            "Batch 750, D Loss: 0.6416991353034973, G Loss: 0.80552077293396\n",
            "Batch 800, D Loss: 0.7010596394538879, G Loss: 0.8395628929138184\n",
            "Batch 850, D Loss: 0.628311276435852, G Loss: 0.8302816152572632\n",
            "Batch 900, D Loss: 0.6582379341125488, G Loss: 0.8581554889678955\n",
            "Batch 950, D Loss: 0.6763560771942139, G Loss: 0.857731819152832\n",
            "Batch 1000, D Loss: 0.6428422331809998, G Loss: 0.7243301868438721\n",
            "Batch 1050, D Loss: 0.6203966736793518, G Loss: 0.805364191532135\n",
            "Batch 1100, D Loss: 0.6468585729598999, G Loss: 0.8342719078063965\n",
            "Batch 1150, D Loss: 0.6370128393173218, G Loss: 0.8313065767288208\n",
            "Batch 1200, D Loss: 0.6861519813537598, G Loss: 0.831643283367157\n",
            "Batch 1250, D Loss: 0.7042368650436401, G Loss: 0.8014988899230957\n",
            "Batch 1300, D Loss: 0.6239701509475708, G Loss: 0.8260115385055542\n",
            "Batch 1350, D Loss: 0.6203732490539551, G Loss: 0.8659573793411255\n",
            "Batch 1400, D Loss: 0.638352632522583, G Loss: 0.841510534286499\n",
            "Batch 1450, D Loss: 0.6422353982925415, G Loss: 0.9158496260643005\n",
            "Batch 1500, D Loss: 0.6700724363327026, G Loss: 0.7864001393318176\n",
            "Batch 1550, D Loss: 0.6641740202903748, G Loss: 0.8196573257446289\n",
            "Batch 1600, D Loss: 0.6436395049095154, G Loss: 0.8684089183807373\n",
            "Batch 1650, D Loss: 0.6585147380828857, G Loss: 0.8524848818778992\n",
            "Batch 1700, D Loss: 0.6038203239440918, G Loss: 0.835662305355072\n",
            "Batch 1750, D Loss: 0.6886099576950073, G Loss: 0.8080031275749207\n",
            "Batch 1800, D Loss: 0.693202018737793, G Loss: 0.8982982635498047\n",
            "Batch 1850, D Loss: 0.6657267212867737, G Loss: 0.7383050918579102\n",
            "Epoch 17, D Loss: 0.6547435522079468, G Loss: 0.7997028827667236\n",
            "Starting Epoch 18\n",
            "Batch 0, D Loss: 0.6511831283569336, G Loss: 0.8666103482246399\n",
            "Batch 50, D Loss: 0.5908938646316528, G Loss: 0.8666114211082458\n",
            "Batch 100, D Loss: 0.6771675944328308, G Loss: 0.753969132900238\n",
            "Batch 150, D Loss: 0.6470239162445068, G Loss: 0.7764073014259338\n",
            "Batch 200, D Loss: 0.6527698636054993, G Loss: 0.7423418760299683\n",
            "Batch 250, D Loss: 0.6698668003082275, G Loss: 0.8312643766403198\n",
            "Batch 300, D Loss: 0.6668208837509155, G Loss: 0.7277716398239136\n",
            "Batch 350, D Loss: 0.6637815237045288, G Loss: 0.8049485683441162\n",
            "Batch 400, D Loss: 0.6085041761398315, G Loss: 0.8470441102981567\n",
            "Batch 450, D Loss: 0.5846886038780212, G Loss: 0.8254436254501343\n",
            "Batch 500, D Loss: 0.669036328792572, G Loss: 0.7218451499938965\n",
            "Batch 550, D Loss: 0.6817252039909363, G Loss: 0.6748880743980408\n",
            "Batch 600, D Loss: 0.6941460371017456, G Loss: 0.8797739744186401\n",
            "Batch 650, D Loss: 0.6610925197601318, G Loss: 0.7744550704956055\n",
            "Batch 700, D Loss: 0.6268541812896729, G Loss: 0.7819865942001343\n",
            "Batch 750, D Loss: 0.6761907339096069, G Loss: 0.888401448726654\n",
            "Batch 800, D Loss: 0.6271309852600098, G Loss: 0.8616468906402588\n",
            "Batch 850, D Loss: 0.6335023045539856, G Loss: 0.8579926490783691\n",
            "Batch 900, D Loss: 0.6492272019386292, G Loss: 0.8631188869476318\n",
            "Batch 950, D Loss: 0.6533474326133728, G Loss: 0.8560314178466797\n",
            "Batch 1000, D Loss: 0.6475392580032349, G Loss: 0.8423652648925781\n",
            "Batch 1050, D Loss: 0.6707466244697571, G Loss: 0.8386330008506775\n",
            "Batch 1100, D Loss: 0.6404136419296265, G Loss: 0.8593117594718933\n",
            "Batch 1150, D Loss: 0.6400691270828247, G Loss: 0.7248938083648682\n",
            "Batch 1200, D Loss: 0.7079489231109619, G Loss: 0.7943869829177856\n",
            "Batch 1250, D Loss: 0.6601617336273193, G Loss: 0.8051456212997437\n",
            "Batch 1300, D Loss: 0.6515052318572998, G Loss: 0.8749706745147705\n",
            "Batch 1350, D Loss: 0.627120852470398, G Loss: 0.7926063537597656\n",
            "Batch 1400, D Loss: 0.6318843960762024, G Loss: 0.736229419708252\n",
            "Batch 1450, D Loss: 0.6431210041046143, G Loss: 0.7341793775558472\n",
            "Batch 1500, D Loss: 0.6131300926208496, G Loss: 0.7578096985816956\n",
            "Batch 1550, D Loss: 0.6599085330963135, G Loss: 0.7828078269958496\n",
            "Batch 1600, D Loss: 0.672976016998291, G Loss: 0.7813729643821716\n",
            "Batch 1650, D Loss: 0.7324298620223999, G Loss: 0.838155210018158\n",
            "Batch 1700, D Loss: 0.6218867897987366, G Loss: 0.7337575554847717\n",
            "Batch 1750, D Loss: 0.6390331983566284, G Loss: 0.816406786441803\n",
            "Batch 1800, D Loss: 0.6276780962944031, G Loss: 0.9828444719314575\n",
            "Batch 1850, D Loss: 0.7110643982887268, G Loss: 0.8241533041000366\n",
            "Epoch 18, D Loss: 0.673900842666626, G Loss: 0.8436487913131714\n",
            "Starting Epoch 19\n",
            "Batch 0, D Loss: 0.6719706654548645, G Loss: 0.9220760464668274\n",
            "Batch 50, D Loss: 0.6794993281364441, G Loss: 0.8548706769943237\n",
            "Batch 100, D Loss: 0.6864495277404785, G Loss: 0.8715983629226685\n",
            "Batch 150, D Loss: 0.6437116861343384, G Loss: 0.8096941709518433\n",
            "Batch 200, D Loss: 0.6315689086914062, G Loss: 0.8428486585617065\n",
            "Batch 250, D Loss: 0.6217048168182373, G Loss: 0.7977976202964783\n",
            "Batch 300, D Loss: 0.6543385982513428, G Loss: 0.8812897801399231\n",
            "Batch 350, D Loss: 0.6911957859992981, G Loss: 0.7845070362091064\n",
            "Batch 400, D Loss: 0.7070685625076294, G Loss: 0.7242977619171143\n",
            "Batch 450, D Loss: 0.6267980337142944, G Loss: 0.8729215860366821\n",
            "Batch 500, D Loss: 0.6449507474899292, G Loss: 0.8442619442939758\n",
            "Batch 550, D Loss: 0.6821810007095337, G Loss: 0.8186672925949097\n",
            "Batch 600, D Loss: 0.650998055934906, G Loss: 0.8318586945533752\n",
            "Batch 650, D Loss: 0.639519214630127, G Loss: 0.8530294895172119\n",
            "Batch 700, D Loss: 0.6548177003860474, G Loss: 0.9349879622459412\n",
            "Batch 750, D Loss: 0.6222851276397705, G Loss: 0.8476018905639648\n",
            "Batch 800, D Loss: 0.6770355701446533, G Loss: 0.8218952417373657\n",
            "Batch 850, D Loss: 0.6535168886184692, G Loss: 0.8378095626831055\n",
            "Batch 900, D Loss: 0.6870205998420715, G Loss: 0.8488808870315552\n",
            "Batch 950, D Loss: 0.6304813623428345, G Loss: 0.7706140875816345\n",
            "Batch 1000, D Loss: 0.6529192328453064, G Loss: 0.7907515168190002\n",
            "Batch 1050, D Loss: 0.6784809827804565, G Loss: 0.7606064677238464\n",
            "Batch 1100, D Loss: 0.6628502607345581, G Loss: 0.8297876715660095\n",
            "Batch 1150, D Loss: 0.676379382610321, G Loss: 0.8255266547203064\n",
            "Batch 1200, D Loss: 0.6282590627670288, G Loss: 0.7724250555038452\n",
            "Batch 1250, D Loss: 0.6699326634407043, G Loss: 0.7552562952041626\n",
            "Batch 1300, D Loss: 0.6347789168357849, G Loss: 0.7806397676467896\n",
            "Batch 1350, D Loss: 0.7077898979187012, G Loss: 0.7216209769248962\n",
            "Batch 1400, D Loss: 0.7388075590133667, G Loss: 0.8003835678100586\n",
            "Batch 1450, D Loss: 0.675983726978302, G Loss: 0.8360773324966431\n",
            "Batch 1500, D Loss: 0.6648922562599182, G Loss: 0.8386427760124207\n",
            "Batch 1550, D Loss: 0.7007039785385132, G Loss: 0.8728268146514893\n",
            "Batch 1600, D Loss: 0.6814545392990112, G Loss: 0.9368084669113159\n",
            "Batch 1650, D Loss: 0.6436436772346497, G Loss: 0.9105801582336426\n",
            "Batch 1700, D Loss: 0.6922974586486816, G Loss: 0.7773921489715576\n",
            "Batch 1750, D Loss: 0.6258748769760132, G Loss: 0.7820978164672852\n",
            "Batch 1800, D Loss: 0.6231101155281067, G Loss: 0.8425744771957397\n",
            "Batch 1850, D Loss: 0.6477887034416199, G Loss: 0.8386264443397522\n",
            "Epoch 19, D Loss: 0.6136935949325562, G Loss: 0.8438497185707092\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Dropout, LeakyReLU, Dense\n",
        "from tensorflow.keras.layers import Dense, Reshape, Conv2DTranspose, BatchNormalization\n",
        "from tensorflow.keras.models import Sequential\n",
        "\n",
        "(train_images, _), (_, _) = mnist.load_data()\n",
        "train_images = train_images.reshape(-1, 28, 28, 1).astype('float32') / 127.5 - 1.0  \n",
        "print(\"Train images shape:\", train_images.shape)\n",
        "\n",
        "def build_generator():\n",
        "    model = Sequential([\n",
        "        Dense(7*7*64, input_shape=(100,)), \n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Reshape((7, 7, 64)),\n",
        "        Conv2DTranspose(32, (5, 5), strides=(2, 2), padding='same', use_bias=True),  \n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Conv2DTranspose(1, (5, 5), strides=(2, 2), padding='same', use_bias=True, activation='tanh')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "def build_discriminator():\n",
        "    model = Sequential([\n",
        "        Conv2D(32, (5, 5), strides=(2, 2), padding='same', input_shape=(28, 28, 1), use_bias=True),  \n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.4),  \n",
        "        Conv2D(64, (5, 5), strides=(2, 2), padding='same', use_bias=True),\n",
        "        LeakyReLU(alpha=0.2),\n",
        "        Dropout(0.4),\n",
        "        Flatten(),\n",
        "        Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "generator = build_generator()\n",
        "discriminator = build_discriminator()\n",
        "d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.4, clipnorm=1.0)\n",
        "g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001, beta_1=0.4, clipnorm=1.0)\n",
        "loss_fn = tf.keras.losses.BinaryCrossentropy()\n",
        "@tf.function(input_signature=[\n",
        "    tf.TensorSpec(shape=[None, 28, 28, 1], dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=[None, 28, 28, 1], dtype=tf.float32)\n",
        "])\n",
        "def train_discriminator(real_images, fake_images):\n",
        "    with tf.GradientTape() as tape:\n",
        "        real_predictions = discriminator(real_images, training=True)\n",
        "        fake_predictions = discriminator(fake_images, training=True)\n",
        "        real_loss = loss_fn(tf.ones_like(real_predictions), real_predictions)\n",
        "        fake_loss = loss_fn(tf.zeros_like(fake_predictions), fake_predictions)\n",
        "        d_loss = 0.5 * (real_loss + fake_loss)\n",
        "    gradients = tape.gradient(d_loss, discriminator.trainable_variables)\n",
        "    for grad, var in zip(gradients, discriminator.trainable_variables):\n",
        "        if grad is None:\n",
        "            print(f\"Warning: None gradient for variable {var.name}\")\n",
        "    d_optimizer.apply_gradients(zip(gradients, discriminator.trainable_variables))\n",
        "    return d_loss\n",
        "@tf.function(input_signature=[tf.TensorSpec(shape=(), dtype=tf.int32)])\n",
        "def train_generator(batch_size):\n",
        "    noise = tf.random.normal([batch_size, 100])\n",
        "    with tf.GradientTape() as tape:\n",
        "        fake_images = generator(noise, training=True)\n",
        "        fake_predictions = discriminator(fake_images, training=False)\n",
        "        g_loss = loss_fn(tf.ones_like(fake_predictions), fake_predictions)\n",
        "    gradients = tape.gradient(g_loss, generator.trainable_variables)\n",
        "    for grad, var in zip(gradients, generator.trainable_variables):\n",
        "        if grad is None:\n",
        "            print(f\"Warning: None gradient for variable {var.name}\")\n",
        "    g_optimizer.apply_gradients(zip(gradients, generator.trainable_variables))\n",
        "    return g_loss\n",
        "\n",
        "def save_generated_images(epoch, generator, examples=10):\n",
        "    noise = np.random.normal(0, 1, (examples, 100))\n",
        "    generated = generator.predict(noise, verbose=0)\n",
        "    generated = 0.5 * generated + 0.5\n",
        "    fig, axs = plt.subplots(1, examples, figsize=(10, 1))\n",
        "    for i in range(examples):\n",
        "        axs[i].imshow(generated[i, :, :, 0], cmap='gray')\n",
        "        axs[i].axis('off')\n",
        "    plt.savefig(f\"gan_output_epoch_{epoch}.png\")\n",
        "    plt.close()\n",
        "epochs = 20\n",
        "batch_size = 32  \n",
        "for epoch in range(epochs):\n",
        "    print(f\"Starting Epoch {epoch}\")\n",
        "    for i in range(len(train_images) // batch_size):\n",
        "        real_images = train_images[np.random.randint(0, train_images.shape[0], batch_size)]\n",
        "        noise = np.random.normal(0, 1, (batch_size, 100))\n",
        "        fake_images = generator(noise, training=False)\n",
        "        d_loss = train_discriminator(real_images, fake_images)\n",
        "        g_loss = train_generator(batch_size)\n",
        "\n",
        "        if i % 50 == 0:  \n",
        "            print(f\"Batch {i}, D Loss: {d_loss.numpy()}, G Loss: {g_loss.numpy()}\")\n",
        "\n",
        "    else: \n",
        "        print(f\"Epoch {epoch}, D Loss: {d_loss.numpy()}, G Loss: {g_loss.numpy()}\")\n",
        "        save_generated_images(epoch, generator)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "TPU",
    "colab": {
      "gpuType": "V28",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "gan_env",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
